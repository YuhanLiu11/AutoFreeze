python freeze_e2e_cache.py   --task_name imdb   --do_train   --do_eval   --do_lower_case   --data_dir /mnt/IMDB_data/   --vocab_file /mnt/uncased_L-12_H-768_A-12/vocab.txt --bert_config_file /mnt/uncased_L-12_H-768_A-12/bert_config.json   --init_checkpoint /mnt/uncased_L-12_H-768_A-12/pytorch_model.bin   --max_seq_length 512   --train_batch_size 6   --learning_rate 1e-5   --num_train_epochs 4.0   --output_dir /mnt/test3   --seed 42     --gradient_accumulation_steps 1    --random_seeds 0,1,2,3 --num_intervals 20 --freezing_layers 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5


# python freeze_e2e_cache_test.py   --task_name imdb   --do_train   --do_eval   --do_lower_case   --data_dir /mnt/IMDB_data/   --vocab_file /mnt/uncased_L-12_H-768_A-12/vocab.txt --bert_config_file /mnt/uncased_L-12_H-768_A-12/bert_config.json   --init_checkpoint /mnt/uncased_L-12_H-768_A-12/pytorch_model.bin   --max_seq_length 512   --train_batch_size 6   --learning_rate 1e-5   --num_train_epochs 4.0   --output_dir /mnt/test  --seed 42     --gradient_accumulation_steps 1    --random_seeds 0,1,2,3 --num_intervals 20 --freezing_layers 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
